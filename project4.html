<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Form Intelligence System</title>
  <link
    href="https://fonts.googleapis.com/css2?family=Fira+Code:wght@400;600&family=Bebas+Neue&display=swap"
    rel="stylesheet"
  />
  <link rel="stylesheet" href="style.css" />
</head>
<body class="project-detail">
  <header>
    <div class="project-header-shell">
      <div class="project-header-text">
        <a href="index.html" class="back-link"><span aria-hidden="true">←</span> Back to overview</a>
        <span class="status-chip status-completed">Completed</span>
        <h1 class="project-title">Form Intelligence System</h1>
        <p class="project-tagline">
          A dual-AI document pipeline that processes scanned business forms using OCR, LLMs and a fully local stack for structured field extraction.
        </p>
      </div>
      <div class="project-hero-media">
        <img
          src="images/projectX.png"
          alt="Form processing pipeline architecture"
          class="project-hero-image"
        />
      </div>
    </div>
  </header>

  <main class="project-main">
    <section class="project-metrics-grid">
      <div class="project-metric">
        <p class="metric-title">Team Size</p>
        <p class="metric-value">2 developers</p>
      </div>
      <div class="project-metric">
        <p class="metric-title">Duration</p>
        <p class="metric-value">9 months</p>
      </div>
      <div class="project-metric">
        <p class="metric-title">Partner</p>
        <p class="project-metric">WEB Windenergie AG</p>
      </div>
    </section>

    <section class="project-summary glass-card">
      <h2>Project Overview</h2>
      <p>
        This diploma project aimed to automate the analysis of scanned forms using a hybrid AI system.
        We deployed both text-based and visual models to extract key fields from various PDF layouts
        and stored the results in a structured database. All inference runs locally – no external APIs involved.
      </p>
      <p>
        At its core, the system connects OCR preprocessing, LLM-based field detection,
        a robust RESTful API, and a reactive UI built with Blazor and SignalR. Each component is modular and Dockerized.
      </p>
    </section>

    <section class="project-highlights">
      <article class="highlight-card glass-card">
        <h3>System Highlights</h3>
        <ul>
          <li>OCR preprocessing using Tesseract and pdf2image.</li>
          <li>Text-based field extraction with Qwen 2.5 via Ollama (local LLM).</li>
          <li>Visual form analysis using Donut (Vision Transformer).</li>
          <li>SignalR-powered real-time UI feedback and status tracking.</li>
          <li>Full RESTful backend with MySQL storage and Dockerized services.</li>
        </ul>
      </article>

      <article class="highlight-card glass-card">
        <h3>Tech Stack</h3>
        <ul class="tag-list">
          <li>Python</li>
          <li>Qwen 2.5</li>
          <li>Donut</li>
          <li>Ollama</li>
          <li>Tesseract OCR</li>
          <li>Flask</li>
          <li>C# ASP.NET</li>
          <li>Blazor</li>
          <li>SignalR</li>
          <li>MySQL</li>
          <li>Docker</li>
        </ul>
      </article>

      <article class="highlight-card glass-card">
        <h3>My Contributions</h3>
        <ul>
          <li>Developed the full Python OCR + LLM processing pipeline with local Qwen integration.</li>
          <li>Engineered advanced prompt structures for field extraction from noisy OCR output.</li>
          <li>Built and deployed a responsive Blazor frontend with live API interaction via SignalR.</li>
        </ul>
      </article>
    </section>
  </main>

  <footer>
    <p>&copy; 2025 | Created by Jannik Hofstetter</p>
  </footer>
</body>
</html>
