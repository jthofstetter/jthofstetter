<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Form Intelligence System</title>
  <link
    href="https://fonts.googleapis.com/css2?family=Fira+Code:wght@400;600&family=Bebas+Neue&display=swap"
    rel="stylesheet"
  />
  <link rel="stylesheet" href="style.css" />
</head>
<body class="project-detail">
  <header>
    <div class="project-header-shell">
      <div class="project-header-text">
        <a href="index.html" class="back-link"><span aria-hidden="true">←</span> Back to overview</a>
        <span class="status-chip status-finished">Finished</span>
        <h1 class="project-title">Form Intelligence System</h1>
        <p class="project-tagline">
          A hybrid LLM pipeline that processes scanned documents using OCR, local AI models, and structured field extraction for automated downstream usage.
        </p>
      </div>
      <div class="project-hero-media">
        <img
          src="images/project5.png"
          alt="Architecture diagram for form processing"
          class="project-hero-image"
        />
      </div>
    </div>
  </header>

  <main class="project-main">
    <section class="project-metrics-grid">
      <div class="project-metric">
        <p class="metric-title">Team Size</p>
        <p class="metric-value">2 developers</p>
      </div>
      <div class="project-metric">
        <p class="metric-title">Duration</p>
        <p class="metric-value">9 months</p>
      </div>
      <div class="project-metric">
        <p class="metric-title">Client</p>
        <p class="metric-value">WEB Windenergie AG</p>
      </div>
    </section>

    <section class="project-summary glass-card">
      <h2>Project Overview</h2>
      <p>
        Our diploma thesis aimed to automate the processing of business documents — especially scanned forms — using open-source LLMs and OCR pipelines. 
        The system was fully containerized and operated 100% locally for data privacy.
      </p>
      <p>
        We evaluated multiple models and implemented two parallel AI tracks: one text-based (OCR + Qwen2.5), the other vision-based (Donut Transformer). 
        Data extraction results were processed and saved via a RESTful API and visualized in a modern frontend.
      </p>
    </section>

    <section class="project-highlights">
      <article class="highlight-card glass-card">
        <h3>Pipeline Highlights</h3>
        <ul>
          <li>Local OCR pipeline using Tesseract and pdf2image.</li>
          <li>Text extraction via Qwen 2.5 using Ollama (REST interface).</li>
          <li>Visual document understanding with Donut (Vision Transformer).</li>
          <li>REST API (Flask + C# ASP.NET) with SignalR for real-time updates.</li>
          <li>Structured MySQL storage with Docker-based deployment.</li>
        </ul>
      </article>

      <article class="highlight-card glass-card">
        <h3>Tech Stack</h3>
        <ul class="tag-list">
          <li>Python</li>
          <li>Qwen 2.5</li>
          <li>Ollama</li>
          <li>Donut</li>
          <li>Flask</li>
          <li>Tesseract OCR</li>
          <li>C# ASP.NET</li>
          <li>Blazor</li>
          <li>MySQL</li>
          <li>Docker</li>
        </ul>
      </article>

      <article class="highlight-card glass-card">
        <h3>Key Learnings</h3>
        <ul>
          <li>Combining multimodal AI models creates more robust document intelligence.</li>
          <li>Local inference pipelines can replace cloud APIs in real-world scenarios.</li>
          <li>Prompt engineering is essential for precise field extraction.</li>
        </ul>
      </article>
    </section>
  </main>

  <footer>
    <p>&copy; 2025 | Created by Jannik Hofstetter</p>
  </footer>
</body>
</html>
